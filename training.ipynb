{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlpoetxyLVtOBroKt9sKQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhernandezlordui/Final-Project-LLM/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_qBIhrGRdoa",
        "outputId": "f8964bf8-0f88-4c94-e1c7-06abcd64b46e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDz2Q7uaRToY",
        "outputId": "d47ec1a2-2d7b-4e2e-d13c-a319a5a1ef81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Loading Tokenizer for: emilyalsentzer/Bio_ClinicalBERT\n",
            "‚úÖ Bio_ClinicalBERT English Tokenizer loaded successfully.\n",
            "‚úÖ Tensor lists reset for tokenization.\n",
            "Final Input IDs Length (samples): 1000\n",
            "Final Labels Length (samples): 1000\n",
            "Input Tensor Shape (Input IDs): torch.Size([1000, 512])\n",
            "‚úÖ Data tokenized and split into 80 training batches and 20 validation batches.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# --- PARAMETER CONFIGURATION ---\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/project 2 LLM/generated_narratives_batch_save.csv'\n",
        "MAX_LEN = 512 # Maximum length for the BERT sequence\n",
        "BATCH_SIZE = 10 # Batch size for training\n",
        "NUM_LABELS = 36 # Total number of labels (ASD + ADHD)\n",
        "TEST_SIZE = 0.2 # 20% for validation\n",
        "ENGLISH_MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'\n",
        "\n",
        "# 1. Load the DataFrame\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# 2. Define Label Columns\n",
        "label_cols = [col for col in df.columns if col.startswith('ASD_') or col.startswith('ADHD_')]\n",
        "assert len(label_cols) == NUM_LABELS, \"The number of label columns does not match NUM_LABELS.\"\n",
        "\n",
        "# 3. Prepare Data and Labels for PyTorch\n",
        "texts = df['text'].values\n",
        "labels = df[label_cols].values\n",
        "\n",
        "# 4. Load the Tokenizer\n",
        "try:\n",
        "    print(f\"-> Loading Tokenizer for: {ENGLISH_MODEL_NAME}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ENGLISH_MODEL_NAME)\n",
        "    print(\"‚úÖ Bio_ClinicalBERT English Tokenizer loaded successfully.\")\n",
        "except Exception:\n",
        "    print(\"‚ùå ERROR: Ensure the 'transformers' library is installed or use a valid model name.\")\n",
        "    exit()\n",
        "\n",
        "# 5. Tokenization\n",
        "\n",
        "# Reset on each execution\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "print(\"‚úÖ Tensor lists reset for tokenization.\")\n",
        "# -----------------------------------------------------\n",
        "\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # Document to encode\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = MAX_LEN,\n",
        "                        padding = 'max_length',\n",
        "                        truncation = True, # Added for safety to ensure MAX_LEN\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    # .squeeze(0) converts the shape (1, MAX_LEN) to (MAX_LEN)\n",
        "    input_ids.append(encoded_dict['input_ids'].squeeze(0))\n",
        "    attention_masks.append(encoded_dict['attention_mask'].squeeze(0))\n",
        "\n",
        "# Stacks the list of tensors\n",
        "input_ids = torch.stack(input_ids, dim=0)\n",
        "attention_masks = torch.stack(attention_masks, dim=0)\n",
        "\n",
        "# Labels should already be (1000, 36)\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "print(f\"Final Input IDs Length (samples): {input_ids.shape[0]}\")\n",
        "print(f\"Final Labels Length (samples): {labels.shape[0]}\")\n",
        "print(f\"Input Tensor Shape (Input IDs): {input_ids.shape}\") # Should be (1000, 128)\n",
        "\n",
        "assert input_ids.shape[0] == labels.shape[0], \"Error: Input IDs and Labels lengths do not match.\"\n",
        "\n",
        "\n",
        "# 6. Split into Training and Validation Sets\n",
        "all_data = (input_ids, attention_masks, labels)\n",
        "\n",
        "# Unpack the three tensors consistently\n",
        "(\n",
        "    train_inputs,\n",
        "    validation_inputs,\n",
        "    train_masks,\n",
        "    validation_masks,\n",
        "    train_labels,\n",
        "    validation_labels\n",
        ") = train_test_split(\n",
        "    *all_data,\n",
        "    random_state=42,\n",
        "    test_size=TEST_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 7. Create PyTorch Dataloaders\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"‚úÖ Data tokenized and split into {len(train_dataloader)} training batches and {len(validation_dataloader)} validation batches.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REQUIRED DEFINITIONS FROM PREVIOUS CELLS ---\n",
        "# NUM_LABELS = 36\n",
        "# ENGLISH_MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'\n",
        "EPOCHS = 4#5#4 #9 #10 #4\n",
        "LEARNING_RATE = 5e-6 #2e-5\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/Colab Notebooks/project 2 LLM/clinicalbert_final_epoch_model.pt'\n",
        "# train_dataloader, validation_dataloader (already created)\n",
        "# --------------------------------------------------------\n",
        "\n",
        "## 1. Load the BERT Model\n",
        "\n",
        "try:\n",
        "    # Loads the pre-trained BERT model and adds the 36-output classification layer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ENGLISH_MODEL_NAME,\n",
        "        num_labels=NUM_LABELS,\n",
        "        problem_type=\"multi_label_classification\" # CRITICAL: Indicates multi-label classification\n",
        "    )\n",
        "    print(f\"‚úÖ base model: {ENGLISH_MODEL_NAME} with 36-output classification layer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. DEVICE AND OPTIMIZER CONFIGURATION ---\n",
        "\n",
        "# 2a. Define the Device (GPU or CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU.')\n",
        "\n",
        "# 2b. Moves the model to the device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Configuration of the Optimizer and Loss Function\n",
        "# AdamW for Transformer models\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# BCEWithLogitsLoss, which combines Sigmoid and Binary Cross-Entropy\n",
        "# Good for multi-label classification.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzDQsZbOS_ei",
        "outputId": "15056747-7332-4c34-fa4d-25aca0e9ee05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ base model: emilyalsentzer/Bio_ClinicalBERT with 36-output classification layer loaded successfully.\n",
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PARAMETERS ---\n",
        "# The required input parameters are: NUM_LABELS, ENGLISH_MODEL_NAME, EPOCHS, model, device,\n",
        "# train_dataloader, validation_dataloader, optimizer, and loss_fn.\n",
        "# --------------------\n",
        "\n",
        "## 1. EVALUATION FUNCTION for Clinical Metrics\n",
        "\n",
        "def evaluate_model(model, dataloader, device, loss_fn):\n",
        "    \"\"\"Evaluates the model on the dataloader and calculates loss and key metrics.\"\"\"\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move data to the GPU\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Validation Loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Collect data for metrics\n",
        "        all_logits.append(logits.detach().cpu().numpy())\n",
        "        all_labels.append(b_labels.detach().cpu().numpy())\n",
        "\n",
        "    # Concatenate all results\n",
        "    all_logits = np.concatenate(all_logits, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Calculate Average Loss\n",
        "    avg_eval_loss = total_eval_loss / len(dataloader)\n",
        "\n",
        "    # Calculate Probabilities (Apply Sigmoid)\n",
        "    # The sigmoid function is: P = 1 / (1 + exp(-Logit))\n",
        "    probabilities = 1 / (1 + np.exp(-all_logits))\n",
        "\n",
        "    # Get Binary Predictions using a 0.5 threshold\n",
        "    predictions = (probabilities > 0.5).astype(int)\n",
        "\n",
        "    # CALCULATION OF CLINICAL METRICS\n",
        "    f1_macro = f1_score(all_labels, predictions, average='macro', zero_division=0)\n",
        "    f1_micro = f1_score(all_labels, predictions, average='micro', zero_division=0)\n",
        "\n",
        "    # AUC-ROC (macro average)\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(all_labels, probabilities, average='macro')\n",
        "    except ValueError:\n",
        "        auc_roc = 0.0 # Error handling if only one class is present\n",
        "\n",
        "    return avg_eval_loss, f1_macro, f1_micro, auc_roc\n",
        "\n",
        "\n",
        "## 2. FINE-TUNING LOOP\n",
        "\n",
        "print(\"\\n--- STARTING BERT FINE-TUNING AND EVALUATION ---\")\n",
        "\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "    print(f'\\n======== EPOCH {epoch_i + 1} / {EPOCHS} ========')\n",
        "\n",
        "    # 2a. TRAINING MODE\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Move data to the GPU\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Reset gradients, Model call, and Loss calculation\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        loss = loss_fn(outputs.logits, b_labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "    # 2b. EVALUATION MODE\n",
        "    avg_val_loss, f1_macro, f1_micro, auc_roc = evaluate_model(\n",
        "        model,\n",
        "        validation_dataloader,\n",
        "        device,\n",
        "        loss_fn\n",
        "    )\n",
        "\n",
        "    # 2c. PRINT RESULTS AND MONITOR OVERFITTING\n",
        "    print(f\"  Training Loss (Average): {avg_train_loss:.4f}\")\n",
        "    print(f\"  Validation Loss (Average): {avg_val_loss:.4f} üî•\")\n",
        "    print(f\"  F1-Macro (Validation): {f1_macro:.4f} üî¨\")\n",
        "    print(f\"  AUC-ROC (Validation): {auc_roc:.4f}\")\n",
        "\n",
        "print(\"\\n--- FINE-TUNING COMPLETE ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9tY8CGBTLeh",
        "outputId": "cb3e4159-ed9f-4678-be0b-6782d0daea17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STARTING BERT FINE-TUNING AND EVALUATION ---\n",
            "\n",
            "======== EPOCH 1 / 4 ========\n",
            "  Training Loss (Average): 0.7012\n",
            "  Validation Loss (Average): 0.6898 üî•\n",
            "  F1-Macro (Validation): 0.2769 üî¨\n",
            "  AUC-ROC (Validation): 0.5354\n",
            "\n",
            "======== EPOCH 2 / 4 ========\n",
            "  Training Loss (Average): 0.6718\n",
            "  Validation Loss (Average): 0.6646 üî•\n",
            "  F1-Macro (Validation): 0.4057 üî¨\n",
            "  AUC-ROC (Validation): 0.6261\n",
            "\n",
            "======== EPOCH 3 / 4 ========\n",
            "  Training Loss (Average): 0.6477\n",
            "  Validation Loss (Average): 0.6401 üî•\n",
            "  F1-Macro (Validation): 0.5315 üî¨\n",
            "  AUC-ROC (Validation): 0.7028\n",
            "\n",
            "======== EPOCH 4 / 4 ========\n",
            "  Training Loss (Average): 0.6257\n",
            "  Validation Loss (Average): 0.6280 üî•\n",
            "  F1-Macro (Validation): 0.6107 üî¨\n",
            "  AUC-ROC (Validation): 0.7170\n",
            "\n",
            "--- FINE-TUNING COMPLETE ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "2VSiFBKEZLR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Saving the model ---\n",
        "# state_dict() contains all the learned weights and biases.\n",
        "# ------------------------\n",
        "try:\n",
        "    torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
        "    print(f\"\\n‚úÖ Model weights from the latest epoch saved in: {FINAL_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error saving the model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmEqX5OAZEhU",
        "outputId": "1b571748-1fad-45ee-8edf-0e8e5242c78c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Model weights from the latest epoch saved in: /content/drive/MyDrive/Colab Notebooks/project 2 LLM/clinicalbert_final_epoch_model.pt\n"
          ]
        }
      ]
    }
  ]
}